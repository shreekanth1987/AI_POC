{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1983e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8845b971",
   "metadata": {},
   "source": [
    "# Part 1 : read files and store in JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c2a0b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing updated filenames to sqllite_config.json...\n",
      "Part 1: Filenames updated in sqllite_config.json.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuration for folders and JSON file path\n",
    "FOLDERS = [\"BRONZE\", \"SILVER\", \"GOLD\"]\n",
    "JSON_FILE_PATH = \"sqllite_config.json\"\n",
    "\n",
    "# --- 1. Fetch Filenames ---\n",
    "csv_files = {}\n",
    "\n",
    "for folder in FOLDERS:\n",
    "    try:\n",
    "        # List all files ending with .csv in the folder\n",
    "        files = [f for f in os.listdir(folder) if f.endswith(\".csv\")]\n",
    "        csv_files[folder] = files\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Folder '{folder}' not found. Skipping file fetching for this folder.\")\n",
    "        csv_files[folder] = []\n",
    "\n",
    "\n",
    "# --- 2. Prepare 'files' dictionary for JSON update ---\n",
    "file_config = {}\n",
    "\n",
    "# Bronze: Remains single file (first file in the list)\n",
    "if csv_files[\"BRONZE\"]:\n",
    "    file_config[\"bronze_csv_file\"] = csv_files[\"BRONZE\"][0]\n",
    "\n",
    "# Silver: Updated to include ALL files found as a list\n",
    "if csv_files[\"SILVER\"]:\n",
    "    file_config[\"silver_csv_files_list\"] = csv_files[\"SILVER\"]\n",
    "else:\n",
    "    file_config[\"silver_csv_files_list\"] = [] # Ensure the key exists even if empty\n",
    "\n",
    "# CORRECTION: Gold is now updated to include ALL files found as a list\n",
    "if csv_files[\"GOLD\"]:\n",
    "    file_config[\"gold_csv_files_list\"] = csv_files[\"GOLD\"]\n",
    "else:\n",
    "    file_config[\"gold_csv_files_list\"] = [] # Ensure the key exists even if empty\n",
    "\n",
    "\n",
    "# --- 3. Update JSON File with Filenames ---\n",
    "try:\n",
    "    # Read the existing data\n",
    "    with open(JSON_FILE_PATH, 'r') as f:\n",
    "        config_data = json.load(f)\n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    # Initialize if file doesn't exist or is invalid\n",
    "    config_data = {}\n",
    "\n",
    "# Update the 'files' section (keeping other data intact)\n",
    "# Use .update() to merge the new file configurations into the existing 'files' dictionary\n",
    "existing_files = config_data.get(\"files\", {})\n",
    "existing_files.update(file_config)\n",
    "config_data[\"files\"] = existing_files\n",
    "\n",
    "\n",
    "# Write the modified data back to the file\n",
    "print(f\"Writing updated filenames to {JSON_FILE_PATH}...\")\n",
    "with open(JSON_FILE_PATH, 'w') as f:\n",
    "    json.dump(config_data, f, indent=4)\n",
    "\n",
    "print(f\"Part 1: Filenames updated in {JSON_FILE_PATH}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ffd2b4",
   "metadata": {},
   "source": [
    "# Read filenames and create table names dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b7d59e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading existing configuration from sqllite_config.json...\n",
      "Writing updated nested table configuration back to sqllite_config.json...\n",
      "\n",
      "Part 2: Configuration file updated successfully.\n",
      " - Bronze Table(s) Generated: **bronze_eres_flight**\n",
      " - Silver Table(s) Generated: **Ailab_curated_flight, silver_eres_airportcode, silver_eres_flight, silver_eres_seasoncode**\n",
      " - Gold Table(s) Generated: **AILab_Consumption_Customer_traveller, Gold_dimension_flightdata**\n"
     ]
    }
   ],
   "source": [
    "JSON_FILE_PATH = \"sqllite_config.json\"\n",
    "TIER_KEYWORDS = [\"bronze\", \"silver\", \"gold\"]\n",
    "\n",
    "# Function to implement the simplified table naming logic (remains unchanged)\n",
    "def derive_custom_table_name(filename):\n",
    "    \"\"\"\n",
    "    Extracts the table name starting from the tier keyword and includes\n",
    "    the next two underscore-separated words, effectively stopping after the 3rd word.\n",
    "    Example: 'ailab_bronze_eres_flight_data1.csv' -> 'bronze_eres_flight'\n",
    "    \"\"\"\n",
    "    if not filename:\n",
    "        return None\n",
    "\n",
    "    # 1. Remove the extension and convert to lowercase for searching\n",
    "    base_name, _ = os.path.splitext(filename)\n",
    "    base_name_lower = base_name.lower()\n",
    "    \n",
    "    # 2. Find the starting position of the tier keyword\n",
    "    start_index = -1\n",
    "    \n",
    "    for tier in TIER_KEYWORDS:\n",
    "        if tier in base_name_lower:\n",
    "            start_index = base_name_lower.find(tier)\n",
    "            break\n",
    "    \n",
    "    if start_index != -1:\n",
    "        # Start the relevant part of the name from the tier keyword\n",
    "        custom_name = base_name[start_index:]\n",
    "        \n",
    "        # Split by underscore\n",
    "        parts = custom_name.split('_')\n",
    "        \n",
    "        # We take the tier word (parts[0]) + the next two words (parts[1] and parts[2])\n",
    "        final_parts = parts[:3]\n",
    "        \n",
    "        if final_parts:\n",
    "            # Join the first three parts (e.g., \"bronze_eres_flight\")\n",
    "            return \"_\".join(final_parts)\n",
    "        \n",
    "        return custom_name \n",
    "            \n",
    "    # Fallback if no tier keyword found\n",
    "    return base_name\n",
    "\n",
    "# --- 1. Read JSON file to get filenames ---\n",
    "try:\n",
    "    print(f\"Reading existing configuration from {JSON_FILE_PATH}...\")\n",
    "    with open(JSON_FILE_PATH, 'r') as f:\n",
    "        config_data = json.load(f)\n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    print(f\"Error: Could not read or decode JSON file at {JSON_FILE_PATH}. Aborting Part 2.\")\n",
    "    exit()\n",
    "\n",
    "# Safely extract the filenames dictionary\n",
    "file_config = config_data.get(\"files\", {})\n",
    "\n",
    "# --- MODIFIED: Get single file for Bronze, and lists for Silver/Gold ---\n",
    "bronze_file = file_config.get(\"bronze_csv_file\")\n",
    "silver_files_list = file_config.get(\"silver_csv_files_list\", []) # Default to empty list\n",
    "gold_files_list = file_config.get(\"gold_csv_files_list\", [])     # Default to empty list\n",
    "\n",
    "\n",
    "# --- 2. Prepare the NEW Nested Table Dictionaries ---\n",
    "\n",
    "# Initialize the dictionaries for the new nested structures\n",
    "bronze_tables = {}\n",
    "silver_tables = {}\n",
    "gold_tables = {}\n",
    "all_generated_table_names = []\n",
    "\n",
    "\n",
    "# Helper function to process files and populate the respective table dictionary\n",
    "def process_files_for_tier(file_list_or_single_file, tier_key, table_dict):\n",
    "    # Ensure we are working with an iterable list, even if it's a single string\n",
    "    if isinstance(file_list_or_single_file, str):\n",
    "        files_to_process = [file_list_or_single_file]\n",
    "    elif file_list_or_single_file is None:\n",
    "        files_to_process = []\n",
    "    else:\n",
    "        files_to_process = file_list_or_single_file\n",
    "    \n",
    "    tier_generated_names = []\n",
    "    \n",
    "    for file_name in files_to_process:\n",
    "        if file_name:\n",
    "            # Derive the table name using the custom logic\n",
    "            table_name = derive_custom_table_name(file_name)\n",
    "            \n",
    "            # Enforce desired casing for silver (e.g., 'silver_eres_flight')\n",
    "            if tier_key == \"silver\" and table_name.lower().startswith(\"silver\"):\n",
    "                table_name = \"silver\" + table_name[6:]\n",
    "            \n",
    "            # Use the full filename (with extension removed) for the database file name\n",
    "            # NOTE: For simplicity, we use the derived table_name + '.db' as the value\n",
    "            db_file_name = f\"{table_name}.db\"\n",
    "            \n",
    "            # Populate the table dictionary: Key = Table Name, Value = DB File Name\n",
    "            table_dict[table_name] = db_file_name\n",
    "            tier_generated_names.append(table_name)\n",
    "            \n",
    "    return tier_generated_names\n",
    "\n",
    "\n",
    "# Generate configurations for all three tiers\n",
    "bronze_names = process_files_for_tier(bronze_file, \"bronze\", bronze_tables)\n",
    "silver_names = process_files_for_tier(silver_files_list, \"silver\", silver_tables)\n",
    "gold_names = process_files_for_tier(gold_files_list, \"gold\", gold_tables)\n",
    "\n",
    "\n",
    "# --- 3. Update JSON File with New Nested Tables ---\n",
    "\n",
    "# Update the root config_data with the new bronze/silver/gold_tables dictionaries\n",
    "config_data[\"bronze_tables\"] = bronze_tables\n",
    "config_data[\"silver_tables\"] = silver_tables\n",
    "config_data[\"gold_tables\"] = gold_tables\n",
    "\n",
    "# Remove the old generic 'tables' key if it exists (cleanup from prior attempts)\n",
    "if \"tables\" in config_data:\n",
    "    del config_data[\"tables\"]\n",
    "    \n",
    "# Write the modified data back to the file\n",
    "print(f\"Writing updated nested table configuration back to {JSON_FILE_PATH}...\")\n",
    "with open(JSON_FILE_PATH, 'w') as f:\n",
    "    json.dump(config_data, f, indent=4)\n",
    "\n",
    "\n",
    "# --- Print Summary ---\n",
    "print(f\"\\nPart 2: Configuration file updated successfully.\")\n",
    "print(f\" - Bronze Table(s) Generated: **{', '.join(bronze_names) if bronze_names else 'N/A'}**\")\n",
    "print(f\" - Silver Table(s) Generated: **{', '.join(silver_names) if silver_names else 'N/A'}**\")\n",
    "print(f\" - Gold Table(s) Generated: **{', '.join(gold_names) if gold_names else 'N/A'}**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e955d3",
   "metadata": {},
   "source": [
    "# load into SQLLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8174e457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Data Loading Process Started ---\n",
      "✅ Configuration loaded successfully from sqllite_config.json.\n",
      "\n",
      "--- EXECUTING BRONZE LOAD ---\n",
      "\n",
      "--- Processing BRONZE File: ailab_bronze_eres_flight_data1.csv ---\n",
      "Source Path: BRONZE\\ailab_bronze_eres_flight_data1.csv\n",
      "Target DB: bronze_data.db\n",
      "Target Table: bronze_eres_flight\n",
      "   Successfully read 46 rows from CSV.\n",
      "   ✅ Data successfully loaded into table 'bronze_eres_flight' in bronze_data.db.\n",
      "\n",
      "--- EXECUTING SILVER LOAD (Multiple Files) ---\n",
      "\n",
      "--- Processing SILVER File: Ailab_curated_flight.csv ---\n",
      "Source Path: SILVER\\Ailab_curated_flight.csv\n",
      "Target DB: silver_data.db\n",
      "Target Table: silver_curated_flight\n",
      "   Successfully read 50 rows from CSV.\n",
      "   ✅ Data successfully loaded into table 'silver_curated_flight' in silver_data.db.\n",
      "\n",
      "--- Processing SILVER File: ailab_silver_eres_airportcode.csv ---\n",
      "Source Path: SILVER\\ailab_silver_eres_airportcode.csv\n",
      "Target DB: silver_data.db\n",
      "Target Table: silver_flight\n",
      "   Successfully read 100 rows from CSV.\n",
      "   ✅ Data successfully loaded into table 'silver_flight' in silver_data.db.\n",
      "\n",
      "--- Processing SILVER File: AiLab_Silver_eres_flight.csv ---\n",
      "Source Path: SILVER\\AiLab_Silver_eres_flight.csv\n",
      "Target DB: silver_data.db\n",
      "Target Table: silver_flight\n",
      "   Successfully read 50 rows from CSV.\n",
      "   ✅ Data successfully loaded into table 'silver_flight' in silver_data.db.\n",
      "\n",
      "--- Processing SILVER File: ailab_silver_eres_seasoncode.csv ---\n",
      "Source Path: SILVER\\ailab_silver_eres_seasoncode.csv\n",
      "Target DB: silver_data.db\n",
      "Target Table: silver_flight\n",
      "   Successfully read 20 rows from CSV.\n",
      "   ✅ Data successfully loaded into table 'silver_flight' in silver_data.db.\n",
      "\n",
      "--- EXECUTING GOLD LOAD (Multiple Files) ---\n",
      "\n",
      "--- Processing GOLD File: AILab_Consumption_Customer_traveller.csv ---\n",
      "Source Path: GOLD\\AILab_Consumption_Customer_traveller.csv\n",
      "Target DB: gold_data.db\n",
      "Target Table: gold_flight\n",
      "   Successfully read 50 rows from CSV.\n",
      "   ✅ Data successfully loaded into table 'gold_flight' in gold_data.db.\n",
      "\n",
      "--- Processing GOLD File: AILab_Gold_dimension_flightdata.csv ---\n",
      "Source Path: GOLD\\AILab_Gold_dimension_flightdata.csv\n",
      "Target DB: gold_data.db\n",
      "Target Table: gold_flight\n",
      "   Successfully read 50 rows from CSV.\n",
      "   ✅ Data successfully loaded into table 'gold_flight' in gold_data.db.\n",
      "\n",
      "--- Data Loading Process Finished ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import sys \n",
    "\n",
    "# --- Configuration File Path ---\n",
    "CONFIG_FILE_PATH = \"sqllite_config.json\"\n",
    "\n",
    "def load_data_from_csv_to_db():\n",
    "    \"\"\"Reads configuration, sets up logging, and loads specified CSV files into their respective SQLite databases.\"\"\"\n",
    "    \n",
    "    # 1. Load Configuration from JSON\n",
    "    try:\n",
    "        with open(CONFIG_FILE_PATH, 'r') as f:\n",
    "            config = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ FATAL ERROR: Could not load configuration file {CONFIG_FILE_PATH}. Aborting. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Setup Logging ---\n",
    "    \n",
    "    try:\n",
    "        log_file = config['logging']['log_file']\n",
    "    except KeyError:\n",
    "        log_file = \"default_db_operations.log\"\n",
    "        print(f\"⚠️ Warning: 'logging' key not found in config. Using default log file: {log_file}\")\n",
    "    \n",
    "    # Ensure logging is only configured once\n",
    "    if not logging.getLogger('').handlers:\n",
    "        logging.basicConfig(\n",
    "            filename=log_file,\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S',\n",
    "            filemode='a'\n",
    "        )\n",
    "        \n",
    "        console = logging.StreamHandler(sys.stdout)\n",
    "        console.setLevel(logging.INFO)\n",
    "        formatter = logging.Formatter('%(message)s')\n",
    "        console.setFormatter(formatter)\n",
    "        logging.getLogger('').addHandler(console)\n",
    "    \n",
    "    logging.info(f\"--- Data Loading Process Started ---\")\n",
    "    logging.info(f\"✅ Configuration loaded successfully from {CONFIG_FILE_PATH}.\")\n",
    "\n",
    "\n",
    "    # --- 3. Extract Configuration Values ---\n",
    "    \n",
    "    # Files\n",
    "    bronze_filename = config['files'].get('bronze_csv_file')\n",
    "    silver_files = config['files'].get('silver_csv_files_list', [])\n",
    "    gold_files = config['files'].get('gold_csv_files_list', [])\n",
    "    \n",
    "    # Databases\n",
    "    db_map = {\n",
    "        \"bronze\": config['databases']['bronze_database_file'],\n",
    "        \"silver\": config['databases']['silver_database_file'],\n",
    "        \"gold\": config['databases']['gold_database_file'],\n",
    "    }\n",
    "    \n",
    "    # Schemas (for dynamic mapping)\n",
    "    silver_schema = config.get('silver_schema', {})\n",
    "    gold_schema = config.get('gold_schema', {})\n",
    "    \n",
    "    # Tables (The actual table name inside the DB)\n",
    "    try:\n",
    "        bronze_table_name = list(config['bronze_tables'].keys())[0]\n",
    "    except (IndexError, KeyError):\n",
    "        bronze_table_name = None\n",
    "\n",
    "\n",
    "    # ⬇️ --- 4. Schema Mapping and Processing Function (CORRECTED) --- ⬇️\n",
    "    \n",
    "    def get_target_schema_name(tier_key, filename, silver_schema, gold_schema):\n",
    "        \"\"\"Determines the target table name based on the 'curated' keyword, \n",
    "        using values from the respective schema dictionary.\"\"\"\n",
    "        \n",
    "        # Determine the correct schema dictionary based on the tier\n",
    "        schema_dict = silver_schema if tier_key == \"silver\" else gold_schema\n",
    "        \n",
    "        # Keyword used for conditional routing in both file and schema value\n",
    "        CURATION_KEYWORD = \"curated\" \n",
    "        \n",
    "        # Rule 1: If filename contains \"curated\" (case-insensitive)\n",
    "        if CURATION_KEYWORD in filename.lower():\n",
    "            # Find the value in the schema dictionary that contains \"curated\"\n",
    "            for value in schema_dict.values():\n",
    "                if CURATION_KEYWORD in value:\n",
    "                    # ✅ Correctly returns 'silver_curated_flight' or 'gold_curated_flight'\n",
    "                    return value \n",
    "            \n",
    "            logging.warning(f\"⚠️ Curation file '{filename}' found, but no matching '{CURATION_KEYWORD}' schema value in {tier_key}_schema.\")\n",
    "            \n",
    "        # Rule 2: Use the non-curation schema as the default\n",
    "        else:\n",
    "            # Find the value that does NOT contain \"curated\"\n",
    "            for value in schema_dict.values():\n",
    "                 if CURATION_KEYWORD not in value:\n",
    "                     # ✅ Correctly returns 'silver_flight' or 'gold_flight'\n",
    "                     return value \n",
    "                     \n",
    "            logging.warning(f\"⚠️ No default schema found (no schema value without '{CURATION_KEYWORD}') for file '{filename}'.\")\n",
    "            \n",
    "        return None # Return None if mapping fails\n",
    "\n",
    "\n",
    "    def process_data_load(tier_name, folder, csv_file, db_file, table_name):\n",
    "        \"\"\"Loads a single CSV file into a specified SQLite table.\"\"\"\n",
    "        full_csv_path = os.path.join(folder, csv_file)\n",
    "        \n",
    "        logging.info(f\"\\n--- Processing {tier_name.upper()} File: {csv_file} ---\")\n",
    "        logging.info(f\"Source Path: {full_csv_path}\")\n",
    "        logging.info(f\"Target DB: {db_file}\")\n",
    "        logging.info(f\"Target Table: {table_name}\")\n",
    "        \n",
    "        if not os.path.exists(full_csv_path):\n",
    "            logging.error(f\"❌ ERROR: CSV file not found at {full_csv_path}. Skipping load.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(full_csv_path)\n",
    "            logging.info(f\"   Successfully read {len(df)} rows from CSV.\")\n",
    "            \n",
    "            conn = sqlite3.connect(db_file)\n",
    "            \n",
    "            # The table name is the schema value derived above\n",
    "            df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            logging.info(f\"   ✅ Data successfully loaded into table '{table_name}' in {db_file}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"❌ ERROR during {tier_name} data loading for {csv_file}: {e}\")\n",
    "\n",
    "    # --- 5. Execute Data Loading ---\n",
    "    \n",
    "    # A. Load Bronze Data (Single File)\n",
    "    logging.info(\"\\n--- EXECUTING BRONZE LOAD ---\")\n",
    "    if bronze_filename and bronze_table_name:\n",
    "        process_data_load(\n",
    "            tier_name=\"bronze\",\n",
    "            folder=\"BRONZE\",\n",
    "            csv_file=bronze_filename,\n",
    "            db_file=db_map[\"bronze\"],\n",
    "            table_name=bronze_table_name\n",
    "        )\n",
    "\n",
    "    # B. Load Silver Data (Multiple Files with Schema Mapping)\n",
    "    logging.info(\"\\n--- EXECUTING SILVER LOAD (Multiple Files) ---\")\n",
    "    for silver_file in silver_files:\n",
    "        target_table = get_target_schema_name(\"silver\", silver_file, silver_schema, gold_schema)\n",
    "        \n",
    "        if target_table:\n",
    "            process_data_load(\n",
    "                tier_name=\"silver\",\n",
    "                folder=\"SILVER\",\n",
    "                csv_file=silver_file,\n",
    "                db_file=db_map[\"silver\"],\n",
    "                table_name=target_table\n",
    "            )\n",
    "\n",
    "    # C. Load Gold Data (Multiple Files with Schema Mapping)\n",
    "    logging.info(\"\\n--- EXECUTING GOLD LOAD (Multiple Files) ---\")\n",
    "    for gold_file in gold_files:\n",
    "        # Note: We pass silver_schema and gold_schema, but only the appropriate one is used inside the function\n",
    "        target_table = get_target_schema_name(\"gold\", gold_file, silver_schema, gold_schema)\n",
    "        \n",
    "        if target_table:\n",
    "            process_data_load(\n",
    "                tier_name=\"gold\",\n",
    "                folder=\"GOLD\",\n",
    "                csv_file=gold_file,\n",
    "                db_file=db_map[\"gold\"],\n",
    "                table_name=target_table\n",
    "            )\n",
    "    \n",
    "    logging.info(\"\\n--- Data Loading Process Finished ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_data_from_csv_to_db()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
