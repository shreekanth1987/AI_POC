{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee6f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84299552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully connected to database: my_database.db\n",
      "‚úÖ Successfully read CSV file: bmi.csv\n",
      "‚úÖ Successfully uploaded data to table: data_table\n",
      "üìä Rows uploaded: 500\n",
      "‚û°Ô∏è Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "# The name of your CSV file\n",
    "csv_file = 'bmi.csv' \n",
    "# The name of the SQLite database file (it will be created if it doesn't exist)\n",
    "database_file = 'my_database.db' \n",
    "# The name of the table you want to create in the database\n",
    "table_name = 'data_table' \n",
    "# --- End Configuration ---\n",
    "\n",
    "def upload_csv_to_sqlite(csv_path, db_path, table):\n",
    "    \"\"\"\n",
    "    Reads a CSV file and uploads its contents to an SQLite table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Connect to the SQLite database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        print(f\"‚úÖ Successfully connected to database: {db_path}\")\n",
    "\n",
    "        # 2. Read the CSV file into a pandas DataFrame\n",
    "        # 'header=0' means the first row is used for column names\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"‚úÖ Successfully read CSV file: {csv_path}\")\n",
    "\n",
    "        # 3. Write the DataFrame to the SQLite database\n",
    "        # 'if_exists': 'replace' will drop the table and create a new one\n",
    "        # 'if_exists': 'append' will add rows to an existing table\n",
    "        # 'index=False' prevents pandas from writing the DataFrame index as a column\n",
    "        df.to_sql(table, conn, if_exists='replace', index=False)\n",
    "        print(f\"‚úÖ Successfully uploaded data to table: {table}\")\n",
    "        \n",
    "        # Optional: Verify the upload\n",
    "        check_query = f\"SELECT COUNT(*) FROM {table};\"\n",
    "        count = pd.read_sql_query(check_query, conn).iloc[0, 0]\n",
    "        print(f\"üìä Rows uploaded: {count}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: The file '{csv_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An unexpected error occurred: {e}\")\n",
    "    finally:\n",
    "        # 4. Close the connection\n",
    "        if 'conn' in locals() and conn:\n",
    "            conn.close()\n",
    "            print(\"‚û°Ô∏è Database connection closed.\")\n",
    "\n",
    "# Run the function\n",
    "upload_csv_to_sqlite(csv_file, database_file, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04824d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query: SELECT * FROM data_table;\n",
      "\n",
      "Retrieved Data:\n",
      "('Male', 174, 96, 4)\n",
      "('Male', 189, 87, 2)\n",
      "('Female', 185, 110, 4)\n",
      "('Female', 195, 104, 3)\n",
      "('Male', 149, 61, 3)\n",
      "('Male', 189, 104, 3)\n",
      "('Male', 147, 92, 5)\n",
      "('Male', 154, 111, 5)\n",
      "('Male', 174, 90, 3)\n",
      "('Female', 169, 103, 4)\n",
      "('Male', 195, 81, 2)\n",
      "('Female', 159, 80, 4)\n",
      "('Female', 192, 101, 3)\n",
      "('Male', 155, 51, 2)\n",
      "('Male', 191, 79, 2)\n",
      "('Female', 153, 107, 5)\n",
      "('Female', 157, 110, 5)\n",
      "('Male', 140, 129, 5)\n",
      "('Male', 144, 145, 5)\n",
      "('Male', 172, 139, 5)\n",
      "('Male', 157, 110, 5)\n",
      "('Female', 153, 149, 5)\n",
      "('Female', 169, 97, 4)\n",
      "('Male', 185, 139, 5)\n",
      "('Female', 172, 67, 2)\n",
      "('Female', 151, 64, 3)\n",
      "('Male', 190, 95, 3)\n",
      "('Male', 187, 62, 1)\n",
      "('Female', 163, 159, 5)\n",
      "('Male', 179, 152, 5)\n",
      "('Male', 153, 121, 5)\n",
      "('Male', 178, 52, 1)\n",
      "('Female', 195, 65, 1)\n",
      "('Female', 160, 131, 5)\n",
      "('Female', 157, 153, 5)\n",
      "('Female', 189, 132, 4)\n",
      "('Female', 197, 114, 3)\n",
      "('Male', 144, 80, 4)\n",
      "('Female', 171, 152, 5)\n",
      "('Female', 185, 81, 2)\n",
      "('Female', 175, 120, 4)\n",
      "('Female', 149, 108, 5)\n",
      "('Male', 157, 56, 2)\n",
      "('Male', 161, 118, 5)\n",
      "('Female', 182, 126, 4)\n",
      "('Male', 185, 76, 2)\n",
      "('Female', 188, 122, 4)\n",
      "('Male', 181, 111, 4)\n",
      "('Male', 161, 72, 3)\n",
      "('Male', 140, 152, 5)\n",
      "('Female', 168, 135, 5)\n",
      "('Female', 176, 54, 1)\n",
      "('Male', 163, 110, 5)\n",
      "('Male', 172, 105, 4)\n",
      "('Male', 196, 116, 4)\n",
      "('Female', 187, 89, 3)\n",
      "('Male', 172, 92, 4)\n",
      "('Male', 178, 127, 5)\n",
      "('Female', 164, 70, 3)\n",
      "('Male', 143, 88, 5)\n",
      "('Female', 191, 54, 0)\n",
      "('Female', 141, 143, 5)\n",
      "('Male', 193, 54, 0)\n",
      "('Male', 190, 83, 2)\n",
      "('Male', 175, 135, 5)\n",
      "('Female', 179, 158, 5)\n",
      "('Female', 172, 96, 4)\n",
      "('Female', 168, 59, 2)\n",
      "('Female', 164, 82, 4)\n",
      "('Female', 194, 136, 4)\n",
      "('Female', 153, 51, 2)\n",
      "('Male', 178, 117, 4)\n",
      "('Male', 141, 80, 5)\n",
      "('Male', 180, 75, 2)\n",
      "('Female', 185, 100, 3)\n",
      "('Female', 197, 154, 4)\n",
      "('Male', 165, 104, 4)\n",
      "('Female', 168, 90, 4)\n",
      "('Female', 176, 122, 4)\n",
      "('Male', 181, 51, 0)\n",
      "('Male', 164, 75, 3)\n",
      "('Female', 166, 140, 5)\n",
      "('Female', 190, 105, 3)\n",
      "('Male', 186, 118, 4)\n",
      "('Male', 168, 123, 5)\n",
      "('Male', 198, 50, 0)\n",
      "('Female', 175, 141, 5)\n",
      "('Male', 145, 117, 5)\n",
      "('Female', 159, 104, 5)\n",
      "('Female', 185, 140, 5)\n",
      "('Female', 178, 154, 5)\n",
      "('Female', 183, 96, 3)\n",
      "('Female', 194, 111, 3)\n",
      "('Male', 177, 61, 2)\n",
      "('Male', 197, 119, 4)\n",
      "('Female', 170, 156, 5)\n",
      "('Male', 142, 69, 4)\n",
      "('Male', 160, 139, 5)\n",
      "('Male', 195, 69, 1)\n",
      "('Female', 190, 50, 0)\n",
      "('Male', 199, 156, 4)\n",
      "('Male', 154, 105, 5)\n",
      "('Male', 161, 155, 5)\n",
      "('Female', 198, 145, 4)\n",
      "('Female', 192, 140, 4)\n",
      "('Male', 195, 126, 4)\n",
      "('Male', 166, 160, 5)\n",
      "('Male', 159, 154, 5)\n",
      "('Female', 181, 106, 4)\n",
      "('Male', 149, 66, 3)\n",
      "('Female', 150, 70, 4)\n",
      "('Female', 146, 157, 5)\n",
      "('Male', 190, 135, 4)\n",
      "('Female', 192, 90, 2)\n",
      "('Female', 177, 96, 4)\n",
      "('Male', 148, 60, 3)\n",
      "('Female', 165, 57, 2)\n",
      "('Female', 146, 104, 5)\n",
      "('Male', 144, 108, 5)\n",
      "('Female', 176, 156, 5)\n",
      "('Female', 168, 87, 4)\n",
      "('Male', 187, 122, 4)\n",
      "('Male', 187, 138, 4)\n",
      "('Female', 184, 160, 5)\n",
      "('Female', 158, 149, 5)\n",
      "('Male', 158, 96, 4)\n",
      "('Male', 194, 115, 4)\n",
      "('Female', 145, 79, 4)\n",
      "('Male', 182, 151, 5)\n",
      "('Male', 154, 54, 2)\n",
      "('Female', 168, 139, 5)\n",
      "('Female', 187, 70, 2)\n",
      "('Female', 158, 153, 5)\n",
      "('Female', 167, 110, 4)\n",
      "('Female', 171, 155, 5)\n",
      "('Female', 183, 150, 5)\n",
      "('Female', 190, 156, 5)\n",
      "('Male', 194, 108, 3)\n",
      "('Male', 171, 147, 5)\n",
      "('Male', 159, 124, 5)\n",
      "('Female', 169, 54, 2)\n",
      "('Female', 167, 85, 4)\n",
      "('Male', 180, 149, 5)\n",
      "('Male', 163, 123, 5)\n",
      "('Male', 140, 79, 5)\n",
      "('Male', 197, 125, 4)\n",
      "('Male', 194, 106, 3)\n",
      "('Female', 140, 146, 5)\n",
      "('Male', 195, 98, 3)\n",
      "('Female', 168, 115, 3)\n",
      "('Female', 196, 50, 0)\n",
      "('Male', 140, 52, 3)\n",
      "('Female', 150, 60, 3)\n",
      "('Female', 168, 140, 5)\n",
      "('Female', 155, 111, 5)\n",
      "('Female', 179, 103, 4)\n",
      "('Female', 182, 84, 3)\n",
      "('Male', 168, 160, 5)\n",
      "('Female', 187, 102, 3)\n",
      "('Male', 181, 105, 4)\n",
      "('Male', 199, 99, 2)\n",
      "('Female', 184, 76, 2)\n",
      "('Male', 192, 101, 3)\n",
      "('Female', 182, 143, 5)\n",
      "('Female', 172, 111, 4)\n",
      "('Male', 181, 78, 2)\n",
      "('Male', 176, 109, 4)\n",
      "('Female', 156, 106, 5)\n",
      "('Female', 151, 67, 3)\n",
      "('Female', 188, 80, 2)\n",
      "('Male', 187, 136, 4)\n",
      "('Male', 174, 138, 5)\n",
      "('Male', 167, 151, 5)\n",
      "('Female', 196, 131, 4)\n",
      "('Male', 197, 149, 4)\n",
      "('Female', 185, 119, 4)\n",
      "('Female', 170, 102, 4)\n",
      "('Female', 181, 94, 3)\n",
      "('Female', 166, 126, 5)\n",
      "('Male', 188, 100, 3)\n",
      "('Female', 162, 74, 3)\n",
      "('Male', 177, 117, 4)\n",
      "('Male', 162, 97, 4)\n",
      "('Male', 180, 73, 2)\n",
      "('Female', 192, 108, 3)\n",
      "('Male', 165, 80, 3)\n",
      "('Female', 167, 135, 5)\n",
      "('Female', 182, 84, 3)\n",
      "('Female', 161, 134, 5)\n",
      "('Male', 158, 95, 4)\n",
      "('Male', 141, 85, 5)\n",
      "('Male', 154, 100, 5)\n",
      "('Male', 165, 105, 4)\n",
      "('Female', 142, 137, 5)\n",
      "('Male', 141, 94, 5)\n",
      "('Male', 145, 108, 5)\n",
      "('Male', 157, 74, 4)\n",
      "('Female', 177, 117, 4)\n",
      "('Female', 166, 144, 5)\n",
      "('Male', 193, 151, 5)\n",
      "('Male', 184, 57, 1)\n",
      "('Male', 179, 93, 3)\n",
      "('Female', 156, 89, 4)\n",
      "('Male', 182, 104, 4)\n",
      "('Male', 145, 160, 5)\n",
      "('Female', 150, 87, 4)\n",
      "('Male', 145, 99, 5)\n",
      "('Female', 196, 122, 4)\n",
      "('Male', 191, 96, 3)\n",
      "('Female', 148, 67, 4)\n",
      "('Female', 150, 84, 4)\n",
      "('Male', 148, 155, 5)\n",
      "('Female', 153, 146, 5)\n",
      "('Female', 196, 159, 5)\n",
      "('Female', 185, 52, 0)\n",
      "('Female', 171, 131, 5)\n",
      "('Female', 143, 118, 5)\n",
      "('Female', 142, 86, 5)\n",
      "('Female', 141, 126, 5)\n",
      "('Male', 159, 109, 5)\n",
      "('Female', 173, 82, 2)\n",
      "('Male', 183, 138, 5)\n",
      "('Female', 152, 90, 4)\n",
      "('Male', 178, 140, 5)\n",
      "('Male', 188, 54, 0)\n",
      "('Female', 155, 144, 5)\n",
      "('Male', 166, 70, 3)\n",
      "('Male', 188, 123, 4)\n",
      "('Female', 171, 120, 5)\n",
      "('Male', 179, 130, 5)\n",
      "('Female', 186, 137, 4)\n",
      "('Female', 153, 78, 2)\n",
      "('Female', 184, 86, 3)\n",
      "('Female', 177, 81, 3)\n",
      "('Male', 145, 78, 4)\n",
      "('Male', 170, 81, 3)\n",
      "('Male', 181, 141, 5)\n",
      "('Male', 165, 155, 5)\n",
      "('Female', 174, 65, 2)\n",
      "('Female', 146, 110, 5)\n",
      "('Male', 178, 85, 3)\n",
      "('Male', 166, 61, 2)\n",
      "('Male', 191, 62, 1)\n",
      "('Female', 177, 155, 5)\n",
      "('Female', 183, 50, 0)\n",
      "('Male', 151, 114, 5)\n",
      "('Male', 182, 98, 3)\n",
      "('Female', 142, 159, 5)\n",
      "('Female', 188, 90, 3)\n",
      "('Male', 161, 89, 4)\n",
      "('Male', 153, 70, 3)\n",
      "('Male', 140, 143, 5)\n",
      "('Male', 169, 141, 5)\n",
      "('Female', 162, 159, 5)\n",
      "('Male', 183, 147, 5)\n",
      "('Female', 162, 58, 2)\n",
      "('Female', 172, 109, 4)\n",
      "('Female', 150, 119, 5)\n",
      "('Female', 169, 145, 5)\n",
      "('Female', 184, 132, 4)\n",
      "('Male', 159, 104, 5)\n",
      "('Male', 163, 131, 5)\n",
      "('Male', 156, 137, 5)\n",
      "('Female', 157, 52, 2)\n",
      "('Male', 147, 84, 4)\n",
      "('Male', 141, 86, 5)\n",
      "('Male', 173, 139, 5)\n",
      "('Male', 154, 145, 5)\n",
      "('Male', 168, 148, 5)\n",
      "('Male', 168, 50, 1)\n",
      "('Male', 145, 130, 5)\n",
      "('Male', 152, 103, 5)\n",
      "('Female', 187, 121, 4)\n",
      "('Female', 163, 57, 0)\n",
      "('Male', 178, 83, 3)\n",
      "('Female', 187, 94, 3)\n",
      "('Female', 179, 114, 4)\n",
      "('Male', 190, 80, 2)\n",
      "('Male', 172, 75, 3)\n",
      "('Male', 188, 57, 1)\n",
      "('Male', 193, 65, 1)\n",
      "('Female', 147, 126, 5)\n",
      "('Female', 147, 94, 5)\n",
      "('Male', 166, 107, 4)\n",
      "('Female', 192, 139, 4)\n",
      "('Male', 181, 139, 4)\n",
      "('Male', 150, 74, 4)\n",
      "('Male', 178, 160, 5)\n",
      "('Female', 156, 52, 2)\n",
      "('Male', 149, 100, 5)\n",
      "('Male', 156, 74, 4)\n",
      "('Male', 183, 105, 3)\n",
      "('Female', 162, 68, 3)\n",
      "('Female', 165, 83, 4)\n",
      "('Female', 168, 143, 5)\n",
      "('Male', 160, 156, 5)\n",
      "('Female', 169, 88, 2)\n",
      "('Female', 140, 76, 4)\n",
      "('Female', 187, 92, 3)\n",
      "('Male', 151, 82, 4)\n",
      "('Female', 186, 140, 5)\n",
      "('Male', 182, 108, 4)\n",
      "('Male', 188, 81, 2)\n",
      "('Male', 179, 110, 4)\n",
      "('Female', 156, 126, 5)\n",
      "('Male', 188, 114, 4)\n",
      "('Male', 183, 153, 5)\n",
      "('Male', 144, 88, 5)\n",
      "('Male', 196, 69, 1)\n",
      "('Male', 171, 141, 5)\n",
      "('Male', 171, 147, 5)\n",
      "('Female', 180, 156, 5)\n",
      "('Male', 191, 146, 5)\n",
      "('Female', 179, 67, 2)\n",
      "('Female', 180, 60, 2)\n",
      "('Female', 154, 132, 5)\n",
      "('Male', 188, 99, 3)\n",
      "('Male', 142, 135, 5)\n",
      "('Male', 170, 95, 4)\n",
      "('Male', 152, 141, 5)\n",
      "('Female', 190, 118, 4)\n",
      "('Female', 181, 111, 4)\n",
      "('Male', 153, 104, 5)\n",
      "('Male', 187, 140, 5)\n",
      "('Female', 144, 66, 4)\n",
      "('Female', 148, 54, 2)\n",
      "('Female', 199, 92, 2)\n",
      "('Female', 167, 85, 4)\n",
      "('Female', 164, 71, 3)\n",
      "('Female', 185, 102, 3)\n",
      "('Female', 164, 160, 5)\n",
      "('Male', 142, 71, 4)\n",
      "('Male', 165, 68, 2)\n",
      "('Female', 172, 62, 2)\n",
      "('Female', 157, 56, 2)\n",
      "('Male', 155, 57, 2)\n",
      "('Female', 167, 153, 5)\n",
      "('Female', 164, 126, 5)\n",
      "('Female', 189, 125, 4)\n",
      "('Female', 161, 145, 5)\n",
      "('Female', 155, 71, 3)\n",
      "('Female', 171, 118, 4)\n",
      "('Female', 154, 92, 4)\n",
      "('Male', 179, 83, 3)\n",
      "('Male', 170, 115, 4)\n",
      "('Female', 184, 106, 4)\n",
      "('Female', 191, 68, 2)\n",
      "('Male', 162, 58, 2)\n",
      "('Male', 178, 138, 5)\n",
      "('Female', 157, 60, 2)\n",
      "('Male', 184, 83, 2)\n",
      "('Male', 197, 88, 2)\n",
      "('Female', 160, 51, 2)\n",
      "('Male', 184, 153, 5)\n",
      "('Male', 190, 50, 0)\n",
      "('Male', 174, 90, 3)\n",
      "('Female', 189, 124, 4)\n",
      "('Female', 186, 143, 5)\n",
      "('Female', 180, 58, 1)\n",
      "('Female', 186, 148, 4)\n",
      "('Female', 193, 61, 1)\n",
      "('Male', 161, 103, 4)\n",
      "('Female', 151, 158, 5)\n",
      "('Female', 195, 147, 4)\n",
      "('Female', 184, 152, 5)\n",
      "('Male', 141, 80, 5)\n",
      "('Female', 185, 94, 3)\n",
      "('Female', 186, 127, 4)\n",
      "('Male', 142, 131, 5)\n",
      "('Female', 147, 67, 4)\n",
      "('Male', 151, 62, 3)\n",
      "('Female', 160, 124, 5)\n",
      "('Male', 185, 60, 1)\n",
      "('Female', 163, 63, 2)\n",
      "('Male', 174, 95, 4)\n",
      "('Female', 150, 144, 5)\n",
      "('Male', 142, 91, 5)\n",
      "('Male', 178, 142, 5)\n",
      "('Female', 154, 96, 5)\n",
      "('Male', 176, 87, 3)\n",
      "('Male', 159, 120, 5)\n",
      "('Male', 191, 62, 1)\n",
      "('Male', 177, 117, 4)\n",
      "('Male', 151, 154, 5)\n",
      "('Female', 182, 149, 5)\n",
      "('Female', 197, 72, 2)\n",
      "('Male', 146, 138, 5)\n",
      "('Female', 160, 83, 4)\n",
      "('Female', 157, 66, 3)\n",
      "('Female', 150, 50, 2)\n",
      "('Female', 167, 58, 2)\n",
      "('Female', 180, 70, 2)\n",
      "('Female', 183, 76, 2)\n",
      "('Female', 183, 87, 3)\n",
      "('Female', 152, 154, 5)\n",
      "('Female', 164, 71, 3)\n",
      "('Male', 187, 96, 3)\n",
      "('Male', 169, 136, 5)\n",
      "('Female', 149, 61, 3)\n",
      "('Male', 163, 137, 5)\n",
      "('Female', 195, 104, 3)\n",
      "('Male', 174, 107, 4)\n",
      "('Male', 182, 70, 2)\n",
      "('Male', 169, 110, 4)\n",
      "('Male', 193, 130, 4)\n",
      "('Male', 148, 141, 5)\n",
      "('Male', 186, 68, 2)\n",
      "('Male', 165, 143, 5)\n",
      "('Female', 146, 123, 5)\n",
      "('Female', 166, 133, 5)\n",
      "('Male', 179, 56, 1)\n",
      "('Female', 177, 101, 4)\n",
      "('Male', 181, 154, 5)\n",
      "('Female', 161, 154, 5)\n",
      "('Female', 157, 103, 5)\n",
      "('Female', 169, 98, 4)\n",
      "('Female', 152, 114, 5)\n",
      "('Female', 162, 64, 2)\n",
      "('Male', 162, 130, 5)\n",
      "('Female', 177, 61, 2)\n",
      "('Female', 195, 61, 1)\n",
      "('Male', 140, 146, 5)\n",
      "('Female', 186, 146, 5)\n",
      "('Female', 178, 107, 4)\n",
      "('Male', 174, 54, 1)\n",
      "('Female', 180, 59, 1)\n",
      "('Male', 188, 141, 4)\n",
      "('Female', 187, 130, 4)\n",
      "('Female', 153, 77, 4)\n",
      "('Female', 165, 95, 4)\n",
      "('Female', 178, 79, 2)\n",
      "('Female', 163, 154, 5)\n",
      "('Female', 150, 97, 5)\n",
      "('Male', 179, 127, 4)\n",
      "('Male', 165, 62, 2)\n",
      "('Male', 168, 158, 5)\n",
      "('Female', 153, 133, 5)\n",
      "('Male', 184, 157, 5)\n",
      "('Male', 188, 65, 1)\n",
      "('Female', 166, 153, 5)\n",
      "('Female', 172, 116, 4)\n",
      "('Male', 182, 73, 2)\n",
      "('Male', 143, 149, 5)\n",
      "('Male', 152, 146, 5)\n",
      "('Female', 186, 128, 4)\n",
      "('Male', 159, 140, 5)\n",
      "('Male', 146, 70, 4)\n",
      "('Female', 176, 121, 4)\n",
      "('Female', 146, 101, 5)\n",
      "('Male', 159, 145, 5)\n",
      "('Male', 162, 157, 5)\n",
      "('Female', 172, 90, 4)\n",
      "('Female', 169, 121, 5)\n",
      "('Male', 182, 50, 0)\n",
      "('Female', 183, 79, 2)\n",
      "('Male', 176, 77, 2)\n",
      "('Female', 188, 128, 4)\n",
      "('Female', 175, 83, 2)\n",
      "('Male', 154, 81, 4)\n",
      "('Female', 184, 147, 5)\n",
      "('Male', 179, 123, 4)\n",
      "('Male', 152, 132, 5)\n",
      "('Male', 179, 56, 1)\n",
      "('Female', 145, 141, 5)\n",
      "('Female', 181, 80, 2)\n",
      "('Male', 158, 127, 5)\n",
      "('Female', 188, 99, 3)\n",
      "('Male', 145, 142, 5)\n",
      "('Male', 161, 115, 5)\n",
      "('Male', 198, 109, 3)\n",
      "('Male', 147, 142, 5)\n",
      "('Male', 154, 112, 5)\n",
      "('Female', 178, 65, 2)\n",
      "('Male', 195, 153, 5)\n",
      "('Female', 167, 79, 3)\n",
      "('Male', 183, 131, 4)\n",
      "('Female', 164, 142, 5)\n",
      "('Male', 167, 64, 2)\n",
      "('Female', 151, 55, 2)\n",
      "('Female', 147, 107, 5)\n",
      "('Female', 155, 115, 5)\n",
      "('Female', 172, 108, 4)\n",
      "('Female', 142, 86, 5)\n",
      "('Male', 146, 85, 4)\n",
      "('Female', 188, 115, 4)\n",
      "('Male', 173, 111, 4)\n",
      "('Female', 160, 109, 5)\n",
      "('Male', 187, 80, 2)\n",
      "('Male', 198, 136, 4)\n",
      "('Female', 179, 150, 5)\n",
      "('Female', 164, 59, 2)\n",
      "('Female', 146, 147, 5)\n",
      "('Female', 198, 50, 0)\n",
      "('Female', 170, 53, 1)\n",
      "('Male', 152, 98, 5)\n",
      "('Female', 150, 153, 5)\n",
      "('Female', 184, 121, 4)\n",
      "('Female', 141, 136, 5)\n",
      "('Male', 150, 95, 5)\n",
      "('Male', 173, 131, 5)\n",
      "\n",
      "‚û°Ô∏è Database connection closed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "database_file = 'my_database.db' # The database file you want to connect to\n",
    "table_name = 'data_table' # The table you want to query\n",
    "\n",
    "def retrieve_data_sqlite3(db_path, table):\n",
    "    \"\"\"\n",
    "    Connects to the database, executes a SELECT query, and prints the results.\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        # 1. Connect to the database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        # Create a cursor object to execute SQL commands\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # 2. Define the SQL query\n",
    "        # This query selects ALL columns from the specified table\n",
    "        query = f\"SELECT * FROM {table};\"\n",
    "        \n",
    "        print(f\"Executing query: {query}\")\n",
    "        \n",
    "        # 3. Execute the query\n",
    "        cursor.execute(query)\n",
    "        \n",
    "        # 4. Fetch the results\n",
    "        # .fetchall() retrieves all rows of the query result set\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        # 5. Print the results\n",
    "        if rows:\n",
    "            print(\"\\nRetrieved Data:\")\n",
    "            for row in rows:\n",
    "                print(row)\n",
    "        else:\n",
    "            print(\"No data found in the table.\")\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"‚ùå SQLite error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An unexpected error occurred: {e}\")\n",
    "    finally:\n",
    "        # 6. Close the connection\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            print(\"\\n‚û°Ô∏è Database connection closed.\")\n",
    "\n",
    "# Run the function\n",
    "retrieve_data_sqlite3(database_file, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab81092e",
   "metadata": {},
   "source": [
    "# Insert into sqllite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23a565fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c2c707",
   "metadata": {},
   "source": [
    "## Part 1 : read files and store in JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7012ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing updated filenames to sqllite_config.json...\n",
      "Part 1: Filenames updated in sqllite_config.json.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuration for folders and JSON file path\n",
    "FOLDERS = [\"BRONZE\", \"SILVER\", \"GOLD\"]\n",
    "JSON_FILE_PATH = \"sqllite_config.json\"\n",
    "\n",
    "# --- 1. Fetch Filenames ---\n",
    "csv_files = {}\n",
    "\n",
    "for folder in FOLDERS:\n",
    "    try:\n",
    "        # List all files ending with .csv in the folder\n",
    "        files = [f for f in os.listdir(folder) if f.endswith(\".csv\")]\n",
    "        csv_files[folder] = files\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Folder '{folder}' not found. Skipping file fetching for this folder.\")\n",
    "        csv_files[folder] = []\n",
    "\n",
    "\n",
    "# --- 2. Prepare 'files' dictionary for JSON update ---\n",
    "file_config = {}\n",
    "\n",
    "# Bronze: Remains single file (first file in the list)\n",
    "if csv_files[\"BRONZE\"]:\n",
    "    file_config[\"bronze_csv_file\"] = csv_files[\"BRONZE\"][0]\n",
    "\n",
    "# Silver: Updated to include ALL files found as a list\n",
    "if csv_files[\"SILVER\"]:\n",
    "    file_config[\"silver_csv_files_list\"] = csv_files[\"SILVER\"]\n",
    "else:\n",
    "    file_config[\"silver_csv_files_list\"] = [] # Ensure the key exists even if empty\n",
    "\n",
    "# CORRECTION: Gold is now updated to include ALL files found as a list\n",
    "if csv_files[\"GOLD\"]:\n",
    "    file_config[\"gold_csv_files_list\"] = csv_files[\"GOLD\"]\n",
    "else:\n",
    "    file_config[\"gold_csv_files_list\"] = [] # Ensure the key exists even if empty\n",
    "\n",
    "\n",
    "# --- 3. Update JSON File with Filenames ---\n",
    "try:\n",
    "    # Read the existing data\n",
    "    with open(JSON_FILE_PATH, 'r') as f:\n",
    "        config_data = json.load(f)\n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    # Initialize if file doesn't exist or is invalid\n",
    "    config_data = {}\n",
    "\n",
    "# Update the 'files' section (keeping other data intact)\n",
    "# Use .update() to merge the new file configurations into the existing 'files' dictionary\n",
    "existing_files = config_data.get(\"files\", {})\n",
    "existing_files.update(file_config)\n",
    "config_data[\"files\"] = existing_files\n",
    "\n",
    "\n",
    "# Write the modified data back to the file\n",
    "print(f\"Writing updated filenames to {JSON_FILE_PATH}...\")\n",
    "with open(JSON_FILE_PATH, 'w') as f:\n",
    "    json.dump(config_data, f, indent=4)\n",
    "\n",
    "print(f\"Part 1: Filenames updated in {JSON_FILE_PATH}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1293e67",
   "metadata": {},
   "source": [
    "## Read filenames and create table names dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817531c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading existing configuration from sqllite_config.json...\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import json\n",
    "\n",
    "# JSON_FILE_PATH = \"sqllite_config.json\"\n",
    "# TIER_KEYWORDS = [\"bronze\", \"silver\", \"gold\"]\n",
    "\n",
    "# # Function to implement the simplified table naming logic\n",
    "# def derive_custom_table_name(filename):\n",
    "#     \"\"\"\n",
    "#     Extracts the table name starting from the tier keyword and includes\n",
    "#     the next two underscore-separated words, effectively stopping after the 3rd word.\n",
    "#     Example: 'ailab_bronze_eres_flight_data1.csv' -> 'bronze_eres_flight'\n",
    "#     \"\"\"\n",
    "#     if not filename:\n",
    "#         return None\n",
    "\n",
    "#     # 1. Remove the extension and convert to lowercase for searching\n",
    "#     base_name, _ = os.path.splitext(filename)\n",
    "#     base_name_lower = base_name.lower()\n",
    "    \n",
    "#     # 2. Find the starting position of the tier keyword\n",
    "#     start_index = -1\n",
    "    \n",
    "#     for tier in TIER_KEYWORDS:\n",
    "#         if tier in base_name_lower:\n",
    "#             start_index = base_name_lower.find(tier)\n",
    "#             break\n",
    "    \n",
    "#     if start_index != -1:\n",
    "#         # Start the relevant part of the name from the tier keyword\n",
    "#         custom_name = base_name[start_index:]\n",
    "        \n",
    "#         # Split by underscore\n",
    "#         parts = custom_name.split('_')\n",
    "        \n",
    "#         # --- MODIFIED LOGIC HERE ---\n",
    "#         # We need the tier word (parts[0]) + the next two words (parts[1] and parts[2])\n",
    "#         # We use a slice [:3] to ensure we only take the first three elements.\n",
    "#         final_parts = parts[:3]\n",
    "        \n",
    "#         # Ensure we have at least one part before joining\n",
    "#         if final_parts:\n",
    "#             # Join the first three parts (e.g., \"bronze_eres_flight\")\n",
    "#             return \"_\".join(final_parts)\n",
    "        \n",
    "#         # Fallback to the whole string starting from the tier\n",
    "#         return custom_name \n",
    "            \n",
    "#     # Fallback if no tier keyword found\n",
    "#     return base_name\n",
    "\n",
    "# # --- 1. Read JSON file to get filenames ---\n",
    "# try:\n",
    "#     print(f\"Reading existing configuration from {JSON_FILE_PATH}...\")\n",
    "#     with open(JSON_FILE_PATH, 'r') as f:\n",
    "#         config_data = json.load(f)\n",
    "# except (FileNotFoundError, json.JSONDecodeError):\n",
    "#     print(f\"Error: Could not read or decode JSON file at {JSON_FILE_PATH}. Aborting Part 2.\")\n",
    "#     exit()\n",
    "\n",
    "# # Safely extract the filenames dictionary\n",
    "# file_config = config_data.get(\"files\", {})\n",
    "\n",
    "# # Get the full filenames\n",
    "# bronze_file = file_config.get(\"bronze_csv_file\")\n",
    "# silver_file = file_config.get(\"silver_csv_file\")\n",
    "# gold_file = file_config.get(\"gold_csv_file\")\n",
    "\n",
    "\n",
    "# # --- 2. Prepare the NEW Nested Table Dictionaries ---\n",
    "\n",
    "# # Initialize the dictionary to hold the new nested structures\n",
    "# new_table_config = {}\n",
    "\n",
    "# # Helper function to generate and append the table structure\n",
    "# def append_table_config(file_path, tier_key):\n",
    "#     if file_path:\n",
    "#         table_name = derive_custom_table_name(file_path)\n",
    "        \n",
    "#         # Enforce desired casing for silver (e.g., 'silver_eres_flight')\n",
    "#         if tier_key == \"silver\" and table_name.lower().startswith(\"silver\"):\n",
    "#             table_name = \"silver\" + table_name[6:]\n",
    "            \n",
    "#         db_file_name = f\"{table_name}.db\"\n",
    "#         new_table_config[f\"{tier_key}_tables\"] = {\n",
    "#             table_name: db_file_name\n",
    "#         }\n",
    "#         return table_name # Return the generated name for print feedback\n",
    "#     return None\n",
    "\n",
    "# # Generate and append configurations for all three tiers\n",
    "# bronze_table = append_table_config(bronze_file, \"bronze\")\n",
    "# silver_table = append_table_config(silver_file, \"silver\")\n",
    "# gold_table = append_table_config(gold_file, \"gold\")\n",
    "\n",
    "\n",
    "# # --- 3. Update JSON File with New Nested Tables ---\n",
    "\n",
    "# # Update the root config_data with the new bronze/silver/gold_tables dictionaries\n",
    "# config_data.update(new_table_config)\n",
    "\n",
    "# # Remove the old generic 'tables' key if it exists (cleanup from prior attempts)\n",
    "# if \"tables\" in config_data:\n",
    "#     del config_data[\"tables\"]\n",
    "    \n",
    "# # Write the modified data back to the file\n",
    "# print(f\"Writing updated nested table configuration back to {JSON_FILE_PATH}...\")\n",
    "# with open(JSON_FILE_PATH, 'w') as f:\n",
    "#     json.dump(config_data, f, indent=4)\n",
    "\n",
    "# print(f\"\\nPart 2: Configuration file updated successfully.\")\n",
    "# print(f\" - Bronze Table Name: **{bronze_table if bronze_table else 'N/A'}**\")\n",
    "# print(f\" - Silver Table Name: **{silver_table if silver_table else 'N/A'}**\")\n",
    "# print(f\" - Gold Table Name: **{gold_table if gold_table else 'N/A'}**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8f28af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading existing configuration from sqllite_config.json...\n",
      "Writing updated nested table configuration back to sqllite_config.json...\n",
      "\n",
      "Part 2: Configuration file updated successfully.\n",
      " - Bronze Table(s) Generated: **bronze_eres_flight**\n",
      " - Silver Table(s) Generated: **Ailab_curated_flight, silver_eres_airportcode, silver_eres_flight, silver_eres_seasoncode**\n",
      " - Gold Table(s) Generated: **AILab_Consumption_Customer_traveller, Gold_dimension_flightdata**\n"
     ]
    }
   ],
   "source": [
    "JSON_FILE_PATH = \"sqllite_config.json\"\n",
    "TIER_KEYWORDS = [\"bronze\", \"silver\", \"gold\"]\n",
    "\n",
    "# Function to implement the simplified table naming logic (remains unchanged)\n",
    "def derive_custom_table_name(filename):\n",
    "    \"\"\"\n",
    "    Extracts the table name starting from the tier keyword and includes\n",
    "    the next two underscore-separated words, effectively stopping after the 3rd word.\n",
    "    Example: 'ailab_bronze_eres_flight_data1.csv' -> 'bronze_eres_flight'\n",
    "    \"\"\"\n",
    "    if not filename:\n",
    "        return None\n",
    "\n",
    "    # 1. Remove the extension and convert to lowercase for searching\n",
    "    base_name, _ = os.path.splitext(filename)\n",
    "    base_name_lower = base_name.lower()\n",
    "    \n",
    "    # 2. Find the starting position of the tier keyword\n",
    "    start_index = -1\n",
    "    \n",
    "    for tier in TIER_KEYWORDS:\n",
    "        if tier in base_name_lower:\n",
    "            start_index = base_name_lower.find(tier)\n",
    "            break\n",
    "    \n",
    "    if start_index != -1:\n",
    "        # Start the relevant part of the name from the tier keyword\n",
    "        custom_name = base_name[start_index:]\n",
    "        \n",
    "        # Split by underscore\n",
    "        parts = custom_name.split('_')\n",
    "        \n",
    "        # We take the tier word (parts[0]) + the next two words (parts[1] and parts[2])\n",
    "        final_parts = parts[:3]\n",
    "        \n",
    "        if final_parts:\n",
    "            # Join the first three parts (e.g., \"bronze_eres_flight\")\n",
    "            return \"_\".join(final_parts)\n",
    "        \n",
    "        return custom_name \n",
    "            \n",
    "    # Fallback if no tier keyword found\n",
    "    return base_name\n",
    "\n",
    "# --- 1. Read JSON file to get filenames ---\n",
    "try:\n",
    "    print(f\"Reading existing configuration from {JSON_FILE_PATH}...\")\n",
    "    with open(JSON_FILE_PATH, 'r') as f:\n",
    "        config_data = json.load(f)\n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    print(f\"Error: Could not read or decode JSON file at {JSON_FILE_PATH}. Aborting Part 2.\")\n",
    "    exit()\n",
    "\n",
    "# Safely extract the filenames dictionary\n",
    "file_config = config_data.get(\"files\", {})\n",
    "\n",
    "# --- MODIFIED: Get single file for Bronze, and lists for Silver/Gold ---\n",
    "bronze_file = file_config.get(\"bronze_csv_file\")\n",
    "silver_files_list = file_config.get(\"silver_csv_files_list\", []) # Default to empty list\n",
    "gold_files_list = file_config.get(\"gold_csv_files_list\", [])     # Default to empty list\n",
    "\n",
    "\n",
    "# --- 2. Prepare the NEW Nested Table Dictionaries ---\n",
    "\n",
    "# Initialize the dictionaries for the new nested structures\n",
    "bronze_tables = {}\n",
    "silver_tables = {}\n",
    "gold_tables = {}\n",
    "all_generated_table_names = []\n",
    "\n",
    "\n",
    "# Helper function to process files and populate the respective table dictionary\n",
    "def process_files_for_tier(file_list_or_single_file, tier_key, table_dict):\n",
    "    # Ensure we are working with an iterable list, even if it's a single string\n",
    "    if isinstance(file_list_or_single_file, str):\n",
    "        files_to_process = [file_list_or_single_file]\n",
    "    elif file_list_or_single_file is None:\n",
    "        files_to_process = []\n",
    "    else:\n",
    "        files_to_process = file_list_or_single_file\n",
    "    \n",
    "    tier_generated_names = []\n",
    "    \n",
    "    for file_name in files_to_process:\n",
    "        if file_name:\n",
    "            # Derive the table name using the custom logic\n",
    "            table_name = derive_custom_table_name(file_name)\n",
    "            \n",
    "            # Enforce desired casing for silver (e.g., 'silver_eres_flight')\n",
    "            if tier_key == \"silver\" and table_name.lower().startswith(\"silver\"):\n",
    "                table_name = \"silver\" + table_name[6:]\n",
    "            \n",
    "            # Use the full filename (with extension removed) for the database file name\n",
    "            # NOTE: For simplicity, we use the derived table_name + '.db' as the value\n",
    "            db_file_name = f\"{table_name}.db\"\n",
    "            \n",
    "            # Populate the table dictionary: Key = Table Name, Value = DB File Name\n",
    "            table_dict[table_name] = db_file_name\n",
    "            tier_generated_names.append(table_name)\n",
    "            \n",
    "    return tier_generated_names\n",
    "\n",
    "\n",
    "# Generate configurations for all three tiers\n",
    "bronze_names = process_files_for_tier(bronze_file, \"bronze\", bronze_tables)\n",
    "silver_names = process_files_for_tier(silver_files_list, \"silver\", silver_tables)\n",
    "gold_names = process_files_for_tier(gold_files_list, \"gold\", gold_tables)\n",
    "\n",
    "\n",
    "# --- 3. Update JSON File with New Nested Tables ---\n",
    "\n",
    "# Update the root config_data with the new bronze/silver/gold_tables dictionaries\n",
    "config_data[\"bronze_tables\"] = bronze_tables\n",
    "config_data[\"silver_tables\"] = silver_tables\n",
    "config_data[\"gold_tables\"] = gold_tables\n",
    "\n",
    "# Remove the old generic 'tables' key if it exists (cleanup from prior attempts)\n",
    "if \"tables\" in config_data:\n",
    "    del config_data[\"tables\"]\n",
    "    \n",
    "# Write the modified data back to the file\n",
    "print(f\"Writing updated nested table configuration back to {JSON_FILE_PATH}...\")\n",
    "with open(JSON_FILE_PATH, 'w') as f:\n",
    "    json.dump(config_data, f, indent=4)\n",
    "\n",
    "\n",
    "# --- Print Summary ---\n",
    "print(f\"\\nPart 2: Configuration file updated successfully.\")\n",
    "print(f\" - Bronze Table(s) Generated: **{', '.join(bronze_names) if bronze_names else 'N/A'}**\")\n",
    "print(f\" - Silver Table(s) Generated: **{', '.join(silver_names) if silver_names else 'N/A'}**\")\n",
    "print(f\" - Gold Table(s) Generated: **{', '.join(gold_names) if gold_names else 'N/A'}**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5726bea",
   "metadata": {},
   "source": [
    "## load into SQLLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0630c74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Data Loading Process Started ---\n",
      "--- Data Loading Process Started ---\n",
      "‚úÖ Configuration loaded successfully from sqllite_config.json.\n",
      "‚úÖ Configuration loaded successfully from sqllite_config.json.\n",
      "\n",
      "--- EXECUTING BRONZE LOAD ---\n",
      "\n",
      "--- EXECUTING BRONZE LOAD ---\n",
      "\n",
      "--- Processing BRONZE File: ailab_bronze_eres_flight_data1.csv ---\n",
      "\n",
      "--- Processing BRONZE File: ailab_bronze_eres_flight_data1.csv ---\n",
      "Source Path: BRONZE\\ailab_bronze_eres_flight_data1.csv\n",
      "Source Path: BRONZE\\ailab_bronze_eres_flight_data1.csv\n",
      "Target DB: bronze_data.db\n",
      "Target DB: bronze_data.db\n",
      "Target Table: bronze_eres_flight\n",
      "Target Table: bronze_eres_flight\n",
      "   Successfully read 46 rows from CSV.\n",
      "   Successfully read 46 rows from CSV.\n",
      "   ‚úÖ Data successfully loaded into table 'bronze_eres_flight' in bronze_data.db.\n",
      "   ‚úÖ Data successfully loaded into table 'bronze_eres_flight' in bronze_data.db.\n",
      "\n",
      "--- EXECUTING SILVER LOAD (Multiple Files) ---\n",
      "\n",
      "--- EXECUTING SILVER LOAD (Multiple Files) ---\n",
      "\n",
      "--- Processing SILVER File: Ailab_curated_flight.csv ---\n",
      "\n",
      "--- Processing SILVER File: Ailab_curated_flight.csv ---\n",
      "Source Path: SILVER\\Ailab_curated_flight.csv\n",
      "Source Path: SILVER\\Ailab_curated_flight.csv\n",
      "Target DB: silver_data.db\n",
      "Target DB: silver_data.db\n",
      "Target Table: silver_curation_flight\n",
      "Target Table: silver_curation_flight\n",
      "   Successfully read 50 rows from CSV.\n",
      "   Successfully read 50 rows from CSV.\n",
      "   ‚úÖ Data successfully loaded into table 'silver_curation_flight' in silver_data.db.\n",
      "   ‚úÖ Data successfully loaded into table 'silver_curation_flight' in silver_data.db.\n",
      "\n",
      "--- Processing SILVER File: ailab_silver_eres_airportcode.csv ---\n",
      "\n",
      "--- Processing SILVER File: ailab_silver_eres_airportcode.csv ---\n",
      "Source Path: SILVER\\ailab_silver_eres_airportcode.csv\n",
      "Source Path: SILVER\\ailab_silver_eres_airportcode.csv\n",
      "Target DB: silver_data.db\n",
      "Target DB: silver_data.db\n",
      "Target Table: silver_flight\n",
      "Target Table: silver_flight\n",
      "   Successfully read 100 rows from CSV.\n",
      "   Successfully read 100 rows from CSV.\n",
      "   ‚úÖ Data successfully loaded into table 'silver_flight' in silver_data.db.\n",
      "   ‚úÖ Data successfully loaded into table 'silver_flight' in silver_data.db.\n",
      "\n",
      "--- Processing SILVER File: AiLab_Silver_eres_flight.csv ---\n",
      "\n",
      "--- Processing SILVER File: AiLab_Silver_eres_flight.csv ---\n",
      "Source Path: SILVER\\AiLab_Silver_eres_flight.csv\n",
      "Source Path: SILVER\\AiLab_Silver_eres_flight.csv\n",
      "Target DB: silver_data.db\n",
      "Target DB: silver_data.db\n",
      "Target Table: silver_flight\n",
      "Target Table: silver_flight\n",
      "   Successfully read 50 rows from CSV.\n",
      "   Successfully read 50 rows from CSV.\n",
      "   ‚úÖ Data successfully loaded into table 'silver_flight' in silver_data.db.\n",
      "   ‚úÖ Data successfully loaded into table 'silver_flight' in silver_data.db.\n",
      "\n",
      "--- Processing SILVER File: ailab_silver_eres_seasoncode.csv ---\n",
      "\n",
      "--- Processing SILVER File: ailab_silver_eres_seasoncode.csv ---\n",
      "Source Path: SILVER\\ailab_silver_eres_seasoncode.csv\n",
      "Source Path: SILVER\\ailab_silver_eres_seasoncode.csv\n",
      "Target DB: silver_data.db\n",
      "Target DB: silver_data.db\n",
      "Target Table: silver_flight\n",
      "Target Table: silver_flight\n",
      "   Successfully read 20 rows from CSV.\n",
      "   Successfully read 20 rows from CSV.\n",
      "   ‚úÖ Data successfully loaded into table 'silver_flight' in silver_data.db.\n",
      "   ‚úÖ Data successfully loaded into table 'silver_flight' in silver_data.db.\n",
      "\n",
      "--- EXECUTING GOLD LOAD (Multiple Files) ---\n",
      "\n",
      "--- EXECUTING GOLD LOAD (Multiple Files) ---\n",
      "\n",
      "--- Processing GOLD File: AILab_Consumption_Customer_traveller.csv ---\n",
      "\n",
      "--- Processing GOLD File: AILab_Consumption_Customer_traveller.csv ---\n",
      "Source Path: GOLD\\AILab_Consumption_Customer_traveller.csv\n",
      "Source Path: GOLD\\AILab_Consumption_Customer_traveller.csv\n",
      "Target DB: gold_data.db\n",
      "Target DB: gold_data.db\n",
      "Target Table: gold_flight\n",
      "Target Table: gold_flight\n",
      "   Successfully read 50 rows from CSV.\n",
      "   Successfully read 50 rows from CSV.\n",
      "   ‚úÖ Data successfully loaded into table 'gold_flight' in gold_data.db.\n",
      "   ‚úÖ Data successfully loaded into table 'gold_flight' in gold_data.db.\n",
      "\n",
      "--- Processing GOLD File: AILab_Gold_dimension_flightdata.csv ---\n",
      "\n",
      "--- Processing GOLD File: AILab_Gold_dimension_flightdata.csv ---\n",
      "Source Path: GOLD\\AILab_Gold_dimension_flightdata.csv\n",
      "Source Path: GOLD\\AILab_Gold_dimension_flightdata.csv\n",
      "Target DB: gold_data.db\n",
      "Target DB: gold_data.db\n",
      "Target Table: gold_flight\n",
      "Target Table: gold_flight\n",
      "   Successfully read 50 rows from CSV.\n",
      "   Successfully read 50 rows from CSV.\n",
      "   ‚úÖ Data successfully loaded into table 'gold_flight' in gold_data.db.\n",
      "   ‚úÖ Data successfully loaded into table 'gold_flight' in gold_data.db.\n",
      "\n",
      "--- Data Loading Process Finished ---\n",
      "\n",
      "--- Data Loading Process Finished ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# --- Configuration File Path ---\n",
    "CONFIG_FILE_PATH = \"sqllite_config.json\"\n",
    "\n",
    "def load_data_from_csv_to_db():\n",
    "    \"\"\"Reads configuration, sets up logging, and loads specified CSV files into their respective SQLite databases.\"\"\"\n",
    "    \n",
    "    # 1. Load Configuration from JSON\n",
    "    try:\n",
    "        with open(CONFIG_FILE_PATH, 'r') as f:\n",
    "            config = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå FATAL ERROR: Could not load configuration file {CONFIG_FILE_PATH}. Aborting. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Setup Logging ---\n",
    "    \n",
    "    try:\n",
    "        log_file = config['logging']['log_file']\n",
    "    except KeyError:\n",
    "        log_file = \"default_db_operations.log\"\n",
    "        print(f\"‚ö†Ô∏è Warning: 'logging' key not found in config. Using default log file: {log_file}\")\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        filename=log_file,\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        filemode='a'\n",
    "    )\n",
    "    \n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(message)s')\n",
    "    console.setFormatter(formatter)\n",
    "    logging.getLogger('').addHandler(console)\n",
    "    \n",
    "    logging.info(f\"--- Data Loading Process Started ---\")\n",
    "    logging.info(f\"‚úÖ Configuration loaded successfully from {CONFIG_FILE_PATH}.\")\n",
    "\n",
    "\n",
    "    # --- 3. Extract Configuration Values ---\n",
    "    \n",
    "    # Files\n",
    "    bronze_filename = config['files'].get('bronze_csv_file')\n",
    "    silver_files = config['files'].get('silver_csv_files_list', [])\n",
    "    gold_files = config['files'].get('gold_csv_files_list', [])\n",
    "    \n",
    "    # Databases\n",
    "    db_map = {\n",
    "        \"bronze\": config['databases']['bronze_database_file'],\n",
    "        \"silver\": config['databases']['silver_database_file'],\n",
    "        \"gold\": config['databases']['gold_database_file'],\n",
    "    }\n",
    "    \n",
    "    # Schemas (for dynamic mapping)\n",
    "    silver_schema = config.get('silver_schema', {})\n",
    "    gold_schema = config.get('gold_schema', {})\n",
    "    \n",
    "    # Tables (The actual table name inside the DB)\n",
    "    bronze_table_name = list(config['bronze_tables'].keys())[0]\n",
    "\n",
    "\n",
    "    # --- 4. Schema Mapping and Processing Function ---\n",
    "    \n",
    "    def get_target_schema_name(tier_key, filename, silver_schema, gold_schema):\n",
    "        \"\"\"Determines the target table name based on the 'curation' keyword.\"\"\"\n",
    "        \n",
    "        # Determine the correct schema dictionary based on the tier\n",
    "        schema_dict = silver_schema if tier_key == \"silver\" else gold_schema\n",
    "        \n",
    "        # Rule: If filename contains \"curated\" (case-insensitive)\n",
    "        if \"curated\" in filename.lower():\n",
    "            # Find the value in the schema dictionary that contains \"curation\"\n",
    "            for value in schema_dict.values():\n",
    "                if \"curation\" in value:\n",
    "                    return value  # e.g., 'silver_curation_flight'\n",
    "            # Fallback if curated file exists but no matching schema is found\n",
    "            logging.warning(f\"‚ö†Ô∏è Curation file '{filename}' found, but no matching curation schema in {tier_key}_schema.\")\n",
    "            \n",
    "        # Rule: Use the non-curation schema as the default\n",
    "        else:\n",
    "            # Assuming the other schema key is the default (e.g., 'silver_flight')\n",
    "            # This is a weak assumption; a stronger rule is better. \n",
    "            # We'll use the first key that does NOT contain \"curation\"\n",
    "            for value in schema_dict.values():\n",
    "                 if \"curation\" not in value:\n",
    "                     return value # e.g., 'silver_flight'\n",
    "            # Fallback if only the curation schema key exists\n",
    "            logging.warning(f\"‚ö†Ô∏è No default schema found for file '{filename}'.\")\n",
    "            \n",
    "        return None # Return None if mapping fails\n",
    "\n",
    "\n",
    "    def process_data_load(tier_name, folder, csv_file, db_file, table_name):\n",
    "        \"\"\"Loads a single CSV file into a specified SQLite table.\"\"\"\n",
    "        full_csv_path = os.path.join(folder, csv_file)\n",
    "        \n",
    "        logging.info(f\"\\n--- Processing {tier_name.upper()} File: {csv_file} ---\")\n",
    "        logging.info(f\"Source Path: {full_csv_path}\")\n",
    "        logging.info(f\"Target DB: {db_file}\")\n",
    "        logging.info(f\"Target Table: {table_name}\") # This is the target table name\n",
    "        \n",
    "        if not os.path.exists(full_csv_path):\n",
    "            logging.error(f\"‚ùå ERROR: CSV file not found at {full_csv_path}. Skipping load.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(full_csv_path)\n",
    "            logging.info(f\"   Successfully read {len(df)} rows from CSV.\")\n",
    "            \n",
    "            conn = sqlite3.connect(db_file)\n",
    "            \n",
    "            # The table name passed to this function is the SCHEMA name from the JSON.\n",
    "            # We will use the *derived table name* (from bronze_tables) for Bronze, \n",
    "            # and the *schema name* for Silver/Gold as the table name.\n",
    "            df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            logging.info(f\"   ‚úÖ Data successfully loaded into table '{table_name}' in {db_file}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"‚ùå ERROR during {tier_name} data loading for {csv_file}: {e}\")\n",
    "\n",
    "    # --- 5. Execute Data Loading ---\n",
    "    \n",
    "    # A. Load Bronze Data (Single File)\n",
    "    logging.info(\"\\n--- EXECUTING BRONZE LOAD ---\")\n",
    "    process_data_load(\n",
    "        tier_name=\"bronze\",\n",
    "        folder=\"BRONZE\",\n",
    "        csv_file=bronze_filename,\n",
    "        db_file=db_map[\"bronze\"],\n",
    "        table_name=bronze_table_name # Uses the single pre-derived table name\n",
    "    )\n",
    "\n",
    "    # B. Load Silver Data (Multiple Files with Schema Mapping)\n",
    "    logging.info(\"\\n--- EXECUTING SILVER LOAD (Multiple Files) ---\")\n",
    "    for silver_file in silver_files:\n",
    "        target_table = get_target_schema_name(\"silver\", silver_file, silver_schema, gold_schema)\n",
    "        \n",
    "        if target_table:\n",
    "            process_data_load(\n",
    "                tier_name=\"silver\",\n",
    "                folder=\"SILVER\",\n",
    "                csv_file=silver_file,\n",
    "                db_file=db_map[\"silver\"],\n",
    "                table_name=target_table # Uses the mapped schema name as the SQL table name\n",
    "            )\n",
    "\n",
    "    # C. Load Gold Data (Multiple Files with Schema Mapping)\n",
    "    logging.info(\"\\n--- EXECUTING GOLD LOAD (Multiple Files) ---\")\n",
    "    for gold_file in gold_files:\n",
    "        target_table = get_target_schema_name(\"gold\", gold_file, silver_schema, gold_schema)\n",
    "        \n",
    "        if target_table:\n",
    "            process_data_load(\n",
    "                tier_name=\"gold\",\n",
    "                folder=\"GOLD\",\n",
    "                csv_file=gold_file,\n",
    "                db_file=db_map[\"gold\"],\n",
    "                table_name=target_table # Uses the mapped schema name as the SQL table name\n",
    "            )\n",
    "    \n",
    "    logging.info(\"\\n--- Data Loading Process Finished ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_data_from_csv_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef9a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52dea69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f55368c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b689243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f2a6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61633ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d94a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8808d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully from sqllite_config.json.\n",
      "\n",
      "--- Processing BRONZE Data ---\n",
      "Source: BRONZE\\ailab_bronze_eres_flight_data1.csv\n",
      "Target DB: bronze_data.db\n",
      "Target Table: bronze_eres_flight\n",
      "   Successfully read 46 rows from CSV.\n",
      "   ‚úÖ Data successfully loaded into table 'bronze_eres_flight' in bronze_data.db.\n",
      "\n",
      "--- Processing SILVER Data ---\n",
      "Source: SILVER\\AiLab_Silver_eres_flight.csv\n",
      "Target DB: silver_data.db\n",
      "Target Table: silver_eres_flight\n",
      "   Successfully read 50 rows from CSV.\n",
      "   ‚úÖ Data successfully loaded into table 'silver_eres_flight' in silver_data.db.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # --- Configuration File Path ---\n",
    "# CONFIG_FILE_PATH = \"sqllite_config.json\"\n",
    "\n",
    "# def load_data_from_csv_to_db():\n",
    "#     \"\"\"Reads configuration and loads specified CSV files into their respective SQLite databases.\"\"\"\n",
    "    \n",
    "#     # 1. Load Configuration from JSON\n",
    "#     try:\n",
    "#         with open(CONFIG_FILE_PATH, 'r') as f:\n",
    "#             config = json.load(f)\n",
    "#         print(f\"Configuration loaded successfully from {CONFIG_FILE_PATH}.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading configuration file: {e}\")\n",
    "#         return\n",
    "\n",
    "#     # --- Extract Configuration Values ---\n",
    "    \n",
    "#     # Files and Folders (Assuming folders match file tier: BRONZE/SILVER)\n",
    "#     bronze_filename = config['files']['bronze_csv_file']\n",
    "#     silver_filename = config['files']['silver_csv_file']\n",
    "    \n",
    "#     # Databases\n",
    "#     bronze_db_file = config['databases']['bronze_database_file']\n",
    "#     silver_db_file = config['databases']['silver_database_file']\n",
    "    \n",
    "#     # Table Names (Keys in the nested dictionaries)\n",
    "#     bronze_table_name = list(config['bronze_tables'].keys())[0]\n",
    "#     silver_table_name = list(config['silver_tables'].keys())[0]\n",
    "\n",
    "#     # --- Processing Function ---\n",
    "\n",
    "#     def process_tier_data(tier_name, folder, csv_file, db_file, table_name):\n",
    "#         \"\"\"Loads a single CSV file into a specified SQLite table.\"\"\"\n",
    "#         full_csv_path = os.path.join(folder, csv_file)\n",
    "        \n",
    "#         print(f\"\\n--- Processing {tier_name.upper()} Data ---\")\n",
    "#         print(f\"Source: {full_csv_path}\")\n",
    "#         print(f\"Target DB: {db_file}\")\n",
    "#         print(f\"Target Table: {table_name}\")\n",
    "\n",
    "#         # Check if the source CSV file exists\n",
    "#         if not os.path.exists(full_csv_path):\n",
    "#             print(f\"ERROR: CSV file not found at {full_csv_path}. Skipping {tier_name} load.\")\n",
    "#             return\n",
    "\n",
    "#         try:\n",
    "#             # 1. Read the CSV file into a pandas DataFrame\n",
    "#             df = pd.read_csv(full_csv_path)\n",
    "#             print(f\"   Successfully read {len(df)} rows from CSV.\")\n",
    "            \n",
    "#             # 2. Connect to the SQLite Database (creates it if it doesn't exist)\n",
    "#             conn = sqlite3.connect(db_file)\n",
    "#             cursor = conn.cursor()\n",
    "\n",
    "#             # 3. Load the DataFrame into the SQLite table\n",
    "#             # 'replace' will overwrite the table if it exists. Use 'append' to add new data.\n",
    "#             # 'fail' will stop if the table exists.\n",
    "#             df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "\n",
    "#             conn.commit()\n",
    "#             conn.close()\n",
    "#             print(f\"Data successfully loaded into table '{table_name}' in {db_file}.\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"ERROR during {tier_name} data loading: {e}\")\n",
    "\n",
    "#     # --- Execute Data Loading for Bronze and Silver Tiers ---\n",
    "    \n",
    "#     # Load Bronze Data\n",
    "#     process_tier_data(\n",
    "#         tier_name=\"bronze\",\n",
    "#         folder=\"BRONZE\", # Assumes your folder is named BRONZE\n",
    "#         csv_file=bronze_filename,\n",
    "#         db_file=bronze_db_file,\n",
    "#         table_name=bronze_table_name\n",
    "#     )\n",
    "\n",
    "#     # Load Silver Data\n",
    "#     process_tier_data(\n",
    "#         tier_name=\"silver\",\n",
    "#         folder=\"SILVER\", # Assumes your folder is named SILVER\n",
    "#         csv_file=silver_filename,\n",
    "#         db_file=silver_db_file,\n",
    "#         table_name=silver_table_name\n",
    "#     )\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     load_data_from_csv_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828f38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Data Loading Process Started ---\n",
      "‚úÖ Configuration loaded successfully from sqllite_config.json.\n",
      "\n",
      "--- Processing BRONZE Data ---\n",
      "Source: BRONZE\\ailab_bronze_eres_flight_data1.csv\n",
      "Target DB: bronze_data.db\n",
      "Target Table: bronze_eres_flight\n",
      "   Successfully read 46 rows from CSV.\n",
      "   ‚úÖ Data successfully loaded into table 'bronze_eres_flight' in bronze_data.db.\n",
      "\n",
      "--- Processing SILVER Data ---\n",
      "Source: SILVER\\AiLab_Silver_eres_flight.csv\n",
      "Target DB: silver_data.db\n",
      "Target Table: silver_eres_flight\n",
      "   Successfully read 50 rows from CSV.\n",
      "   ‚úÖ Data successfully loaded into table 'silver_eres_flight' in silver_data.db.\n",
      "\n",
      "--- Data Loading Process Finished ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # --- Configuration File Path ---\n",
    "# CONFIG_FILE_PATH = \"sqllite_config.json\"\n",
    "\n",
    "# def load_data_from_csv_to_db():\n",
    "#     \"\"\"Reads configuration, sets up logging, and loads specified CSV files into their respective SQLite databases.\"\"\"\n",
    "    \n",
    "#     # 1. Load Configuration from JSON (Temporary load to get log file name)\n",
    "#     try:\n",
    "#         with open(CONFIG_FILE_PATH, 'r') as f:\n",
    "#             config = json.load(f)\n",
    "#     except Exception as e:\n",
    "#         # Cannot log error if logging is not yet set up\n",
    "#         print(f\"‚ùå FATAL ERROR: Could not load configuration file {CONFIG_FILE_PATH}. Aborting. Error: {e}\")\n",
    "#         return\n",
    "\n",
    "#     # --- 2. Setup Logging ---\n",
    "    \n",
    "#     # Get the log file name from the configuration\n",
    "#     try:\n",
    "#         log_file = config['logging']['log_file']\n",
    "#     except KeyError:\n",
    "#         log_file = \"default_db_operations.log\"\n",
    "#         print(f\"‚ö†Ô∏è Warning: 'logging' key not found in config. Using default log file: {log_file}\")\n",
    "    \n",
    "#     # Configure logging to write to the file and output to the console\n",
    "#     logging.basicConfig(\n",
    "#         filename=log_file,\n",
    "#         level=logging.INFO, # Capture INFO and above\n",
    "#         format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "#         datefmt='%Y-%m-%d %H:%M:%S',\n",
    "#         filemode='a' # Append to the log file\n",
    "#     )\n",
    "    \n",
    "#     # Also set up a stream handler to print logs to the console\n",
    "#     console = logging.StreamHandler()\n",
    "#     console.setLevel(logging.INFO)\n",
    "#     formatter = logging.Formatter('%(message)s') # Keep console output simple\n",
    "#     console.setFormatter(formatter)\n",
    "#     logging.getLogger('').addHandler(console)\n",
    "    \n",
    "#     logging.info(f\"--- Data Loading Process Started ---\")\n",
    "#     logging.info(f\"‚úÖ Configuration loaded successfully from {CONFIG_FILE_PATH}.\")\n",
    "\n",
    "\n",
    "#     # --- Extract Configuration Values ---\n",
    "    \n",
    "#     # Files and Folders (Assuming folders match file tier: BRONZE/SILVER)\n",
    "#     bronze_filename = config['files']['bronze_csv_file']\n",
    "#     silver_filename = config['files']['silver_csv_file']\n",
    "    \n",
    "#     # Databases\n",
    "#     bronze_db_file = config['databases']['bronze_database_file']\n",
    "#     silver_db_file = config['databases']['silver_database_file']\n",
    "    \n",
    "#     # Table Names (Keys in the nested dictionaries)\n",
    "#     try:\n",
    "#         bronze_table_name = list(config['bronze_tables'].keys())[0]\n",
    "#         silver_table_name = list(config['silver_tables'].keys())[0]\n",
    "#     except (IndexError, KeyError) as e:\n",
    "#         logging.error(f\"‚ùå ERROR: Table configuration missing or invalid: {e}. Aborting.\")\n",
    "#         return\n",
    "\n",
    "\n",
    "#     # --- Processing Function ---\n",
    "\n",
    "#     def process_tier_data(tier_name, folder, csv_file, db_file, table_name):\n",
    "#         \"\"\"Loads a single CSV file into a specified SQLite table.\"\"\"\n",
    "#         full_csv_path = os.path.join(folder, csv_file)\n",
    "        \n",
    "#         # Replaced print() with logging.info()\n",
    "#         logging.info(f\"\\n--- Processing {tier_name.upper()} Data ---\")\n",
    "#         logging.info(f\"Source: {full_csv_path}\")\n",
    "#         logging.info(f\"Target DB: {db_file}\")\n",
    "#         logging.info(f\"Target Table: {table_name}\")\n",
    "\n",
    "#         # Check if the source CSV file exists\n",
    "#         if not os.path.exists(full_csv_path):\n",
    "#             # Replaced print(f\"ERROR:...\") with logging.error()\n",
    "#             logging.error(f\"‚ùå ERROR: CSV file not found at {full_csv_path}. Skipping {tier_name} load.\")\n",
    "#             return\n",
    "\n",
    "#         try:\n",
    "#             # 1. Read the CSV file into a pandas DataFrame\n",
    "#             df = pd.read_csv(full_csv_path)\n",
    "#             logging.info(f\"   Successfully read {len(df)} rows from CSV.\")\n",
    "            \n",
    "#             # 2. Connect to the SQLite Database (creates it if it doesn't exist)\n",
    "#             conn = sqlite3.connect(db_file)\n",
    "#             # Removed unnecessary cursor = conn.cursor()\n",
    "            \n",
    "#             # 3. Load the DataFrame into the SQLite table\n",
    "#             df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "\n",
    "#             conn.commit()\n",
    "#             conn.close()\n",
    "#             # Replaced print(f\"Data successfully...\") with logging.info()\n",
    "#             logging.info(f\"   ‚úÖ Data successfully loaded into table '{table_name}' in {db_file}.\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             # Replaced print(f\"ERROR during...\") with logging.error()\n",
    "#             logging.error(f\"‚ùå ERROR during {tier_name} data loading: {e}\")\n",
    "\n",
    "#     # --- Execute Data Loading for Bronze and Silver Tiers ---\n",
    "    \n",
    "#     # Load Bronze Data\n",
    "#     process_tier_data(\n",
    "#         tier_name=\"bronze\",\n",
    "#         folder=\"BRONZE\", # Assumes your folder is named BRONZE\n",
    "#         csv_file=bronze_filename,\n",
    "#         db_file=bronze_db_file,\n",
    "#         table_name=bronze_table_name\n",
    "#     )\n",
    "\n",
    "#     # Load Silver Data\n",
    "#     process_tier_data(\n",
    "#         tier_name=\"silver\",\n",
    "#         folder=\"SILVER\", # Assumes your folder is named SILVER\n",
    "#         csv_file=silver_filename,\n",
    "#         db_file=silver_db_file,\n",
    "#         table_name=silver_table_name\n",
    "#     )\n",
    "    \n",
    "#     logging.info(\"\\n--- Data Loading Process Finished ---\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     load_data_from_csv_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26453c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5779aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b72e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b385093c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede9e494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7060fecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a5109cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ailab_bronze_eres.flight_data1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "bronze_csv_file = 'ailab_bronze_eres.flight_data1.csv'\n",
    "silver_csv_file = 'AiLab_Silver_eres_flight.csv'\n",
    "bronze_database_file = 'bronze_data.db'\n",
    "silver_database_file = 'silver_data'\n",
    "table_name = 'data_table'\n",
    "log_file = \"db_operations.log\"\n",
    "# --- End Configuration ---\n",
    "\n",
    "# Split filename and extension\n",
    "filename, _ = os.path.splitext(bronze_csv_file)\n",
    "\n",
    "print(filename)  # Output: ailab_bronze_eres.flight_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc974800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading existing configuration from sqllite_config.json...\n",
      "Writing updated configuration back to sqllite_config.json...\n",
      "\n",
      "‚úÖ Configuration updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Setup (Assuming these lists are populated from your earlier code) ---\n",
    "\n",
    "# Define your folders (required for the file population step)\n",
    "# This setup is needed if you are running the entire script from scratch.\n",
    "# folders = [\"BRONZE\", \"SILVER\", \"GOLD\"]\n",
    "# csv_files = {}\n",
    "# ... (code to populate csv_files)\n",
    "\n",
    "# --- Define Example Data (If running without actual file system access) ---\n",
    "# Assuming these variables hold the lists of files and the derived table names\n",
    "# You will replace this with your actual variables from the previous steps.\n",
    "bronze_files = ['user_data.csv', 'transaction_log.csv']\n",
    "silver_files = ['aggregated_sales.csv']\n",
    "gold_files = ['final_report.csv']\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# --- 2. Define the File and Table Configuration Dictionaries ---\n",
    "\n",
    "# Get the first filename from each list for the 'files' section\n",
    "file_config = {}\n",
    "bronze_csv_file = silver_csv_file = gold_csv_file = None\n",
    "\n",
    "if 'bronze_files' in locals() and bronze_files:\n",
    "    bronze_csv_file = bronze_files[0]\n",
    "    file_config[\"bronze_csv_file\"] = bronze_csv_file\n",
    "\n",
    "if 'silver_files' in locals() and silver_files:\n",
    "    silver_csv_file = silver_files[0]\n",
    "    file_config[\"silver_csv_file\"] = silver_csv_file\n",
    "\n",
    "if 'gold_files' in locals() and gold_files:\n",
    "    gold_csv_file = gold_files[0]\n",
    "    file_config[\"gold_csv_file\"] = gold_csv_file\n",
    "\n",
    "\n",
    "# Define the 'tables' section using os.path.splitext()\n",
    "\n",
    "table_config = {\n",
    "    \"bronze_table\": \"bronze_data.db\", # Database file name (from your prior example)\n",
    "    \"silver_table\": \"silver_data.db\", # Database file name (from your prior example)\n",
    "    \"gold_table\": \"gold_data.db\"     # Database file name (from your prior example)\n",
    "}\n",
    "\n",
    "# Use os.path.splitext to extract the filename for the 'table_name' key\n",
    "\n",
    "if gold_csv_file:\n",
    "    # Split the filename and extension\n",
    "    filename_without_ext, _ = os.path.splitext(gold_csv_file)\n",
    "    # Assign the extracted filename (without the extension) as the table name\n",
    "    table_config[\"table_name\"] = filename_without_ext\n",
    "else:\n",
    "    table_config[\"table_name\"] = \"default_data_table\"\n",
    "\n",
    "# --- 3. Update the JSON File ---\n",
    "\n",
    "JSON_FILE_PATH = \"sqllite_config.json\"\n",
    "\n",
    "try:\n",
    "    # 3a. Read the existing data\n",
    "    print(f\"Reading existing configuration from {JSON_FILE_PATH}...\")\n",
    "    with open(JSON_FILE_PATH, 'r') as f:\n",
    "        config_data = json.load(f)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # Handle case where the file doesn't exist\n",
    "    print(f\"Warning: {JSON_FILE_PATH} not found. Creating a new configuration object.\")\n",
    "    config_data = {}\n",
    "except json.JSONDecodeError:\n",
    "    # Handle case where the file is corrupt or empty\n",
    "    print(f\"Error decoding JSON from {JSON_FILE_PATH}. Starting with an empty configuration object.\")\n",
    "    config_data = {}\n",
    "\n",
    "# 3b. Update the specific keys\n",
    "config_data[\"files\"] = file_config\n",
    "config_data[\"tables\"] = table_config\n",
    "\n",
    "# 3c. Write the modified data back to the file\n",
    "print(f\"Writing updated configuration back to {JSON_FILE_PATH}...\")\n",
    "with open(JSON_FILE_PATH, 'w') as f:\n",
    "    json.dump(config_data, f, indent=4)\n",
    "\n",
    "print(\"\\n‚úÖ Configuration updated successfully.\")\n",
    "print(f\"The final table name assigned to 'table_name' is: **{table_config['table_name']}**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f5f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Logging Setup ---\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Operation counters\n",
    "insert_count = 0\n",
    "update_count = 0\n",
    "delete_count = 0\n",
    "\n",
    "\n",
    "def log_operation(query):\n",
    "    \"\"\"Log each SQL operation into the log file.\"\"\"\n",
    "    logging.info(query)\n",
    "\n",
    "\n",
    "def upload_csv_to_sqlite(csv_path, db_path, table):\n",
    "    global insert_count, update_count, delete_count\n",
    "\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        print(f\"‚úÖ Connected to database: {db_path}\")\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"‚úÖ Loaded CSV file: {csv_path}\")\n",
    "\n",
    "        # Drop & recreate table (your original behaviour)\n",
    "        df.to_sql(table, conn, if_exists='replace', index=False)\n",
    "\n",
    "        # Count INSERTs manually\n",
    "        cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "        row_count = cursor.fetchone()[0]\n",
    "        insert_count = row_count\n",
    "\n",
    "        # Log each INSERT record\n",
    "        for index, row in df.iterrows():\n",
    "            query = f\"INSERT INTO {table} VALUES ({', '.join([repr(x) for x in row.values])});\"\n",
    "            log_operation(query)\n",
    "\n",
    "        print(f\"üìä Rows inserted: {insert_count}\")\n",
    "\n",
    "        # Simulate or count UPDATE and DELETE queries\n",
    "        # (No updates/deletes in your original script, so they remain zero.)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "        print(\"‚û°Ô∏è Database connection closed.\")\n",
    "\n",
    "        # Log summary\n",
    "        summary = (\n",
    "            f\"Run Summary ‚Üí INSERTS={insert_count}, \"\n",
    "            f\"UPDATES={update_count}, DELETES={delete_count}\"\n",
    "        )\n",
    "        logging.info(summary)\n",
    "\n",
    "        print(\"üìÅ Log updated:\", log_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run function\n",
    "upload_csv_to_sqlite(csv_file, database_file, table_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
